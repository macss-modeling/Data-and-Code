---
title: "Model Fitting, Bias, & Variance"
author: "Philip Waggoner, MACS 30100 <br /> University of Chicago"
output: pdf_document
---
```{r}
#install.packages('ggforce')
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)

library(tidyverse)
library(here)

set.seed(1234)
theme_set(theme_minimal())
```

# Statistical learning

> Attempt to summarize relationships between variables by reducing the dimensionality of the data

# Improve Shamwow sales

```{r echo = TRUE, eval = FALSE}
# get advertising data
advertising <- read_csv(here("data", "Advertising.csv")) %>%
  tbl_df() %>%
  select(-X1)
```

```{r echo = TRUE, eval = FALSE}
# plot separate facets for relationship between ad spending and sales
plot_ad <- advertising %>%
  gather(method, spend, -Sales) %>%
  ggplot(aes(spend, Sales)) +
  facet_wrap(~ method, scales = "free_x") +
  geom_point() +
  labs(x = "Spending (in thousands of dollars)"); plot_ad
```

# Parametric methods

```{r echo = TRUE, eval = FALSE}
method_model <- function(df) {
  lm(Sales ~ spend, data = df)
}

ad_pred <- advertising %>%
  gather(method, spend, -Sales) %>%
  group_by(method) %>%
  nest() %>%
  mutate(model = map(data, method_model),
         pred = map(model, broom::augment)) %>%
  unnest(pred)

plot_ad +
  geom_smooth(method = "lm", se = FALSE) +
  geom_linerange(data = ad_pred,
                 aes(ymin = Sales, ymax = .fitted),
                 color = "blue",
                 alpha = .5) 
```

# Non-parameteric: Locally weighted scatterplot smoothing

```{r echo = TRUE, eval = FALSE}
library(broom) # model operations
library(lattice)  # for the data

mod <- loess(NOx ~ E, 
             data = ethanol, 
             degree = 1, 
             span = .75)

fit <- augment(mod)

ggplot(fit, aes(E, NOx)) +
  geom_point() +
  geom_line(aes(y = .fitted), color = "red") +
  labs(x = "Equivalence ratio",
       y = "Concentration of nitrogen oxides")
```

# Optimism of training error

```{r echo = TRUE, eval = FALSE}
# simulate data and fig from ISL 2.9
sim_mse <- tibble(
  x = runif(n = 50, min = 0, max = 100),
  y = 5.055901 - 0.1848551*x + 0.00748706*x^2 - 0.00005543478*x^3 + rnorm(50, sd = 0.6)
)

# model fit
ggplot(sim_mse, aes(x, y)) +
  geom_point() +
  stat_function(fun = function(x) 5.055901 - 0.1848551*x + 0.00748706*x^2 - 0.00005543478*x^3) +
  geom_smooth(aes(color = "lm"), method = lm, se = FALSE) +
  geom_smooth(aes(color = "spline-low"), method = lm,
              formula = y ~ splines::ns(x, 5), se = FALSE) +
  geom_smooth(aes(color = "spline-high"), method = lm,
              formula = y ~ splines::ns(x, 20), se = FALSE) +
  scale_color_brewer(type = "qual") +
  labs(title = "Training data points",
       subtitle = "Models estimated on training set",
       x = expression(X),
       y = expression(Y)) +
  theme(legend.position = "none")
```

Now, the test set.

```{r echo = TRUE, eval = FALSE}
sim_mse_test <- tibble(
  x = runif(n = 50, min = 0, max = 100),
  y = 5.055901 - 0.1848551 * x + 0.00748706 * x^2 - 0.00005543478 * x^3 + rnorm(50, sd = 0.6)
)

# model fit
ggplot(sim_mse, aes(x, y)) +
  geom_point(data = sim_mse_test) +
  stat_function(fun = function(x) 5.055901 - 0.1848551*x + 0.00748706*x^2 - 0.00005543478*x^3) +
  geom_smooth(aes(color = "lm"), method = lm, se = FALSE) +
  geom_smooth(aes(color = "spline-low"), method = lm,
              formula = y ~ splines::ns(x, 5), se = FALSE) +
  geom_smooth(aes(color = "spline-high"), method = lm,
              formula = y ~ splines::ns(x, 20), se = FALSE) +
  scale_color_brewer(type = "qual") +
  labs(title = "Test data points",
       subtitle = "Models estimated on training set",
       x = expression(X),
       y = expression(Y)) +
  theme(legend.position = "none")
```

# Bias-variance trade-off

```{r echo = TRUE, eval = FALSE}
ggplot(sim_mse, aes(x, y)) +
  geom_point() +
  labs(title = "Training data",
       x = expression(X),
       y = expression(Y))
```


```{r echo = TRUE, eval = FALSE}
# estimate
sim_knn1 <- FNN::knn.reg(train = sim_mse,
                         test = sim_mse,
                         y = sim_mse$y,
                         k = 1)

# plot
sim_mse %>%
  mutate(pred = sim_knn1$pred) %>%
  ggplot(aes(x, y)) +
  geom_point() +
  geom_step(aes(y = pred)) +
  labs(title = "Training data",
       x = expression(X),
       y = expression(Y))
```


```{r echo = TRUE, eval = FALSE}
ggplot(sim_mse, aes(x, y)) +
  geom_point() +
  geom_hline(yintercept = mean(sim_mse$y)) +
  labs(title = "Training data",
       x = expression(X),
       y = expression(Y))
```


```{r echo = TRUE, eval = FALSE}
# set number of throws
n_games <- 100

# throw `n_games` of darts and get the coordinates where they hit
dart_game <- function(n_games, 
                       accurate = TRUE, 
                       consistent = TRUE) {
  if (accurate & consistent) {
    xvals <- rnorm(n_games, mean = 0, sd = .05)
    yvals <- rnorm(n_games, mean = 0, sd = .05)
  } else if (accurate == TRUE & consistent == FALSE) {
    xvals <- rnorm(n_games, mean = .5, sd = .05)
    yvals <- rnorm(n_games, mean = .4, sd = .05)
  } else if (accurate == FALSE & consistent == TRUE) {
    xvals <- rnorm(n_games, mean = 0, sd = .3)
    yvals <- rnorm(n_games, mean = 0, sd = .3)
  } else if (accurate == FALSE & consistent == FALSE) {
    xvals <- rnorm(n_games, mean = .5, sd = .3)
    yvals <- rnorm(n_games, mean = -.4, sd = .3)
  }
  
  tibble(
    x = xvals,
    y = yvals,
    accurate = accurate,
    consistent = consistent
  )
}

# get data for each situation
throws <- bind_rows(
  dart_game(n_games, accurate = TRUE, consistent = TRUE),
  dart_game(n_games, accurate = TRUE, consistent = FALSE),
  dart_game(n_games, accurate = FALSE, consistent = TRUE),
  dart_game(n_games, accurate = FALSE, consistent = FALSE)
) %>%
  mutate(
    accurate = ifelse(accurate, "Low Variance", "High Variance"),
    consistent = ifelse(consistent, "Low Bias", "High Bias")
  )

# plot the dart board, facet by each type
ggplot(data = throws, aes(x, y)) +
  facet_grid(accurate ~ consistent) +
  ggforce::geom_circle(aes(x = NULL, y = NULL, x0 = 0, y0 = 0, r = 1)) +
  geom_point(alpha = 0.5) +
  xlim(-1, 1) +
  ylim(-1, 1) +
  coord_fixed() +
  labs(title = NULL,
       x = NULL,
       y = NULL) +
  annotate("point", x = 0, y = 0, size = 3)
```

\newpage

# On your own

For this section, you will work in small groups of 4-5. *I will create these groups at random*. 

**IMPORTANT**: _Don't forget that this code you're working on here is due at the appropriate Canvas module (in the form of an attachment to a "Discussion" post) prior to 5:00 pm CDT today. You need only submit a **single** file/script to be considered for credit (i.e., this .Rmd with your code inserted below each question). Recall, I don't care whether you got things right. I only care that attempts to each question have been made._ 

We will now walk through some of the techniques covered this week and last, but this time using real data. Specifically, for this set of exercises, you will use the 2016 American National Election Pilot Study (ANES). Load the data:

```{r echo = TRUE, eval = FALSE}
library(tidyverse)
library(here)

anes <- read_csv(here("data", "anes_2016.csv"))
```

With the data loaded, answer the following questions. The objective here is twofold: 1) to practice your statistical computing skills, and 2) apply and explore error from fitting models on different sets of data.

1. Using some of the techniques we covered last week:

    a. Select only the Obama feeling thermometer (`ftobama`), household income (`faminc`), party affiliation on a 3 point scale (`pid3`), birth year (`birthyr`), and gender (`gender`) (*be sure to recode missing values to `NA` and omit these*)
  
```{r echo = TRUE, eval = FALSE}

dfnew <- anes %>%
  select(ftobama, faminc, pid3, birthyr, gender) %>%
  mutate(ftobama = replace(ftobama, ftobama == 998, NA)) %>%
  filter(is.na(ftobama)==FALSE)
  
```

    b. Split the subset data into training (75%) and testing (25%) sets (*hint*: remember to set the seed (`set.seed()`) prior to creating the split, as the proportions are generated at random)
  
```{r echo = TRUE, eval = FALSE}
#install.packages("tidymodels")
library(tidymodels)

set.seed(1234)
split_tidy <- initial_split(dfnew,
                            prop = 0.75) # default is 75%

# take a look...
split_tidy

# create
train <- training(split_tidy)
test  <- testing(split_tidy)

```
    c. Plot the distributions of each against each other to ensure they look similar
```{r echo = TRUE, eval = FALSE}
# viz
tidy <- train %>% 
  ggplot(aes(x = ftobama)) + 
  geom_line(stat = "density") + 
  geom_line(data = test, 
            stat = "density", 
            col = "red") +
  labs(title = "Via Tidymodels") +
  theme_minimal()

tidy
```

2. Fit a linear regression (`lm()`) on the *training* data, predicting Obama approval as a function of all other features.
```{r echo = TRUE, eval = FALSE}

model <- lm(ftobama~., data=train)
summary(model)

```




3. Calculate the training mean squared error (*hint*: consider using the `mse()` function from Dr. Soltoff's `rcfss` package, which is at the uc-cfss github, *not* on CRAN).

```{r}
#install.packages("remotes")
#remotes::install_github("uc-cfss/rcfss")
library(rcfss)

mse(train, train$ftobama, predict(model, train))


```


4. Calculate predictions for the testing set, using the model you built on the training set (*hint*: consider either `predict()` from base R, or `augment()` from `broom`).

```{r}

test_pred <- predict(model, test)

```

5. Calculate the testing mean squared error.
```{r}

print(mse(test, test$ftobama, test_pred))


```

6. Compare the mean squared error from both sets numerically, side-by-side. What do you see? *Discuss in your groups and record a few sentences as a response.*

Response:
The MSE for testing data is larger than MSE for training data.
It makes sense because we are fitting the linear model on training set.



7. Write your own function to calculate the MSE. Then, use it to re-answer questions 3 and 5. Present the results here, and compare with the `rcfss` approach via `mse()`. These results should be identical to the `mse()` version. Are they? If not, *why* do you think? (*just a sentence or two on your general thoughts if they differ*) 


```{r}
mymse <- function(data, truth, estimate) {
  mse = sum((truth-estimate)^2)/dim(data)[1]
  return(mse)
}

#train
print(mymse(train, train$ftobama, predict(model, train)))

```
```{r}
#test
print(mymse(test, test$ftobama, predict(model, test)))
```

The result is identical!



