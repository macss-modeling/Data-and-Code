---
title: "Bootstrapping + Regression, pt. 1"
author: "Philip Waggoner, MACS 30100 <br /> University of Chicago"
output: pdf_document
---

# Overview

Today: 

  1. Bootstrapping (uncertainty around mean and $\beta$)
  2. Basic regression modeling in R (fitting, interpreting, plotting, and conditional relationships)

# The Bootstrap

**Task**: how often Americans eat ice cream in a given month.

**Sub-task**: check distributional assumptions of the *likely* distribution of these data: Poisson distribution

The probability mass function (PMF) for the Poisson distribution, $$\Pr(X = x) = e^{-\lambda} \frac{\lambda^{k}}{k!},$$

where $\lambda$ is the event rate, $e$ is Euler's number, $k$ is an integer with range $[0, \infty]$. 


Bootstrapping (from lecture):

  1. Draw $B$ samples **with replacement** from the sample (of size $N$)
  2. (For our task) calculate the mean of the bootstrapped sample means $\hat{\mu}_1, \hat{\mu}_2, \dots, \hat{\mu}_B$
  3. Estimate the standard error (SE) of the sample mean $\hat{\mu}$, $SE_{B}(\hat{\mu}) = \sqrt{\frac{1}{B-1} \sum_{r = 1}^{B} \left( \hat{\mu}_r - \frac{1}{B} \sum_{r' = 1}^{B} \hat{\mu}_{r'} \right)^2}$

## Application

Let's see this in action.

```{r}
library(tidyverse)
library(tidymodels)

# set up data
set.seed(1234)

mu <- 5
n_obs <- 1000
ice <- tibble(sim = rpois(n_obs, lambda = mu))

mu_samp <- mean(ice$sim) 
sem <- sqrt(mu_samp / n_obs) 

# Bootstrap

## helper fun
mean_ice <- function(splits) {
  x <- analysis(splits)
  mean(x$sim)
}

ice_boot <- ice %>% 
  bootstraps(1000) %>% 
  mutate(mean = map_dbl(splits, mean_ice))

boot_sem <- sd(ice_boot$mean)

# compare
tibble(sem, boot_sem) 
```

Now, plot.

```{r}
ggplot(ice_boot, aes(mean)) +
  geom_histogram(binwidth = .01, alpha = 0.25) +
  geom_vline(aes(xintercept = mu, color = "Population mean"), size = 1) +
  geom_vline(aes(xintercept = mu_samp, color = "Sample mean"), size = 1) +
  geom_vline(aes(xintercept = mean(mean),
                 color = "Bootstrapped mean"), size = 1) +
  geom_vline(aes(xintercept = mean(mean) + 1.96 * boot_sem,
                 color = "Bootstrapped mean"), linetype = 2) +
  geom_vline(aes(xintercept = mean(mean) - 1.96 * boot_sem,
                 color = "Bootstrapped mean"), linetype = 2) +
  geom_vline(aes(xintercept = mu_samp + 1.96 * sem, color = "Sample mean"),
             linetype = 2) +
  geom_vline(aes(xintercept = mu_samp - 1.96 * sem, color = "Sample mean"),
             linetype = 2) +
  scale_color_brewer(type = "qual",
                     name = NULL,
                     breaks = c("Population mean", "Sample mean",
                                "Bootstrapped mean")) +
  labs(x = "Bootstrapped sample mean",
       y = "Count") +
  theme(legend.position = "bottom")
```

Now, let's break the process and violate the Poisson assumptions. 

```{r}
# break it
set.seed(113)

ice2 <- tibble(sim = c(rpois(n_obs / 2, lambda = mu),
                       round(runif(n_obs / 2, min = 0, max = 10))))

# new calcs
mu2_samp <- mean(ice2$sim)
sem2 <- sqrt(mu2_samp / n_obs)

# bootstrapping
ice2_boot <- ice2 %>%
  bootstraps(1000) %>%
  mutate(mean = map_dbl(splits, mean_ice))

boot2_sem <- sd(ice2_boot$mean)

# plot
ggplot(ice2_boot, aes(mean)) +
  geom_histogram(binwidth = .01, alpha = 0.25) +
  geom_vline(aes(xintercept = mu, color = "Population mean"), size = 1) +
  geom_vline(aes(xintercept = mu2_samp, color = "Sample mean"), size = 1) +
  geom_vline(aes(xintercept = mean(mean),
                 color = "Bootstrapped mean"), size = 1) +
  geom_vline(aes(xintercept = mean(mean) + 1.96 * boot2_sem,
                 color = "Bootstrapped mean"), linetype = 2) +
  geom_vline(aes(xintercept = mean(mean) - 1.96 * boot2_sem,
                 color = "Bootstrapped mean"), linetype = 2) +
  geom_vline(aes(xintercept = mu2_samp + 1.96 * sem2, color = "Sample mean"),
             linetype = 2) +
  geom_vline(aes(xintercept = mu2_samp - 1.96 * sem2, color = "Sample mean"),
             linetype = 2) +
  scale_color_brewer(type = "qual",
                     name = NULL,
                     breaks = c("Population mean", "Sample mean",
                                "Bootstrapped mean")) +
  labs(x = "Bootstrapped sample mean",
       y = "Count") +
  theme_minimal()
```

# Estimating the accuracy of a linear regression model

**Task**: calculate uncertainty around coefficient estimates from linear regression

Back to the `horsepower` and `mpg` linear model via the `Auto` dataset.

```{r}
library(ISLR)

Auto <- as_tibble(Auto)

# descriptive plot
ggplot(Auto, aes(horsepower, mpg)) +
  geom_point() +
  geom_smooth(method = "lm") + 
  theme_minimal()

# SLM
auto_lm <- lm(mpg ~ poly(horsepower, 1, raw = TRUE), data = Auto); tidy(auto_lm)

# Bootstrap
lm_coefs <- function(splits, ...) {
  mod <- lm(..., data = analysis(splits))
  tidy(mod)
}

auto_boot <- Auto %>%
  bootstraps(1000) %>%
  mutate(coef = map(splits, lm_coefs, as.formula(mpg ~ poly(horsepower, 1, raw = TRUE))))

# calc and compare
auto_boot %>%
  unnest(coef) %>%
  group_by(term) %>%
  summarize(.estimate = mean(estimate),
            .se = sd(estimate, na.rm = TRUE))
```

# Regression & INXN

Explore basic linear models in R and conditional relationships via 2008 NES data.

````{r nes-data}
library(tidyverse)
library(foreign)
library(skimr)
library(broom)
library(modelr)
library(here)

set.seed(1234)
theme_set(theme_minimal())

# get nes data
nes <- read.dta(here("data", "nes2008.dta")) %>%
  select(obama_therm_post, partyid3, libcon7, libcon7_obama) %>%
  mutate_all(funs(ifelse(is.nan(.), NA, .))) %>%
  rename(ObamaTherm = obama_therm_post,
         RConserv = libcon7,
         ObamaConserv = libcon7_obama) %>%
  mutate(GOP = ifelse(partyid3 == 3, 1, 0)) %>%
  select(-partyid3) %>%
  na.omit()

# inspect
skim(nes)
```

SLM (simple linear model)

```{r obama-linear}
obama_base <- lm(ObamaTherm ~ RConserv + GOP, 
                 data = nes); tidy(obama_base)
```

# Estimating models with multiplicative interactions

**Expectation**: Varying effects between ideology and party affiliation, with more or less extreme effects of ideology across its range and across party on feelings toward Obama,

$$
\begin{split}
\text{Obama thermometer} = \beta_0 &+ \beta_1 (\text{Respondent conservatism}) + \beta_2 (\text{GOP respondent})\\
& + \beta_3 (\text{Respondent conservatism}) (\text{GOP respondent}) + \epsilon
\end{split}
$$

Fit the model.

```{r obama-ideo-x-gop}
obama_ideo_gop <- lm(ObamaTherm ~ RConserv * GOP, data = nes); tidy(obama_ideo_gop)
```

Now, plot.

```{r obama-ideo-x-gop-plot}
nes %>%
  add_predictions(obama_ideo_gop) %>%
  ggplot(aes(RConserv, ObamaTherm, color = factor(GOP))) +
  geom_jitter(alpha = .5) +
  geom_line(aes(y = pred)) +
  scale_color_manual(values = c("blue", "red")) +
  labs(x = "Respondent conservatism",
       y = "Obama thermometer score") +
  theme(legend.position = "none")
```

Another approach.

```{r obama-split}
tidy(lm(ObamaTherm ~ RConserv, data = filter(nes, GOP == 0)))
tidy(lm(ObamaTherm ~ RConserv, data = filter(nes, GOP == 1)))
```

## Causal direction

Exploring the first difference over party affiliation.

```{r obama-other-direction}
nes %>%
  data_grid(RConserv, GOP) %>%
  add_predictions(obama_ideo_gop) %>%
  spread(GOP, pred) %>%
  mutate(diff = `1` - `0`) %>%
  ggplot(aes(RConserv, diff)) +
  geom_point() +
  labs(title = "Expected Obama thermometer score",
       x = "Respondent conservatism",
       y = "First difference between Republicans\nand non-Republicans")
```

# On your own

For this section, you will work in small groups of 4-5. *I will create these groups at random*. 

**IMPORTANT**: _Don't forget that this code you're working on here is due at the appropriate Canvas module (in the form of an attachment to a "Discussion" post) prior to 5:00 pm CDT tomorrow. You need only submit a **single** file/script to be considered for credit (i.e., this .Rmd with your code inserted below each question). Recall, I don't care whether you got things right. I only care that attempts to each question have been made._ 

Biden feelings data from the ANES. Load with the following code. 

```{r}
library(tidyverse)
library(broom)
library(here)

biden <- read_csv(here("data", "biden.csv"))
```

1. Estimate a **linear** model of the relationship between age (`age`) and attitudes toward Biden (`biden`), and plot the results. Remember to show the 95% confidence interval around your estimated fit line (hint: `geom_smooth()`). For reference, this simple model takes the form, $$\text{Biden}_i = \beta_0 + \beta_1 \text{Age}$$

2. Relax the linear assumption to attempt to account for the fewer observations at the extreme values of `age`, and estimate a fourth-order **polynomial** regression of the relationship between age and attitudes towards Biden (that is, wrap X's in `poly()`), and plot the results. Again, remember to show the 95% confidence interval around your estimated fit line. For reference, the fourth-order polynomial model takes the form, $$\text{Biden}_i = \beta_0 + \beta_1 \text{Age} + \beta_2 \text{Age}^2 + \beta_3 \text{Age}^3 + \beta_4 \text{Age}^4$$

3. In the figure produced in response to the previous question, you plotted the predicted values with the 95% confidence interval. In the case of ordinary linear regression (both the simple and polynomial models in our case), this is easy to estimate. Recall, the **standard error** is a measure of variance for the estimated parameter and is calculated by taking the square root (`sqrt()`) of the diagonal (`diag()`) of the variance-covariance matrix (`vcov()`). Standard errors, which are simply measures of uncertainty around some estimate, are critical to a traditional understanding of "statistical significance," which is reached by diagnosing t-statistics. Of note, t-statistics can be calculated by dividing estimated coefficients (`$coefficients`) by their standard errors. So assuming errors are t-distributed, if these values are greater than 1.96 (the so-called "t-critical value for 95% confidence"), then the estimate is assumed to be "significant" at the 95% confidence level (note: $t > \approx 2.5$ for significance at 99% level). 

    * Obtain the variance-covariance matrix for your polynomial regression, and then calculate (by hand/don't call values from `broom::tidy()`) and report the standard errors for each parameter from your polynomial regression (*to answer this, make sure you read the question carefully*).
    * Calculate (by hand/don't call values from `broom::tidy()`) and report the t-statistics for each of parameter from your polynomial regression (*to answer this, make sure you read the question carefully*).
    * Which coefficient estimates are significant at the 95% level, and which are not? *Hint:* you might consider *comparing* your manually-calculated estimates with those obtained via `broom::tidy()` as a quality check to make sure you followed the steps correctly. What does this substantively mean (a single sentence/thought will do)?

# Appendix: Some extra code for the interested user

If we want to conduct inference on $\hat{\psi}_1$ or $\hat{\psi}_2$ (the marginal effect of either $X$ on $Y$), we can do that as well:

```{r obama-other-dir-std-err}
# function to get coefficient estimates and standard errors
# model -> lm object
# mod_var -> name of moderating variable in the interaction

instant_effect <- function(model, mod_var){
  # get interaction term name
  int.name <- names(model$coefficients)[[which(str_detect(names(model$coefficients), ":"))]]
  
  marg_var <- str_split(int.name, ":")[[1]][[which(str_split(int.name, ":")[[1]] != mod_var)]]
  
  # store coefficients and covariance matrix
  beta.hat <- coef(model)
  cov <- vcov(model)
  
  # possible set of values for mod_var
  if(class(model)[[1]] == "lm"){
    z <- seq(min(model$model[[mod_var]]), max(model$model[[mod_var]]))
  } else {
    z <- seq(min(model$data[[mod_var]]), max(model$data[[mod_var]]))
  }
  
  # calculate instantaneous effect
  dy.dx <- beta.hat[[marg_var]] + beta.hat[[int.name]] * z
  
  # calculate standard errors for instantaeous effect
  se.dy.dx <- sqrt(cov[marg_var, marg_var] +
                     z^2 * cov[int.name, int.name] +
                     2 * z * cov[marg_var, int.name])
  
  # combine into data frame
  tibble(z = z,
             dy.dx = dy.dx,
             se = se.dy.dx)
}

# point range plot
instant_effect(obama_ideo_gop, "RConserv") %>%
  ggplot(aes(z, dy.dx,
             ymin = dy.dx - 1.96 * se,
             ymax = dy.dx + 1.96 * se)) +
  geom_pointrange() +
  geom_hline(yintercept = 0, linetype = 2) +
  labs(title = "Marginal effect of GOP",
       subtitle = "By respondent conservatism",
       x = "Respondent conservatism",
       y = "Estimated marginal effect")

# line plot
instant_effect(obama_ideo_gop, "RConserv") %>%
  ggplot(aes(z, dy.dx)) +
  geom_line() +
  geom_line(aes(y = dy.dx - 1.96 * se), linetype = 2) +
  geom_line(aes(y = dy.dx + 1.96 * se), linetype = 2) +
  geom_hline(yintercept = 0) +
  labs(title = "Marginal effect of GOP",
       subtitle = "By respondent conservatism",
       x = "Respondent conservatism",
       y = "Estimated marginal effect")
```
